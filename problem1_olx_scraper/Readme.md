# ğŸŒ Universal Web Scraper (Python)

**Author:** Vijaya Mishra  
**Purpose:** Demonstrate a **safe, simple, and effective web scraping** approach using Python to extract structured information such as product titles and prices from any HTML-based website.  

It scrapes data from *publicly scrape-friendly* sites like [Books to Scrape](https://books.toscrape.com), a sandbox designed for practicing scraping.

---

## ğŸ§  Why This Method Is Used

This scraper is designed to be:
- âœ… **Lightweight and fast** â€” no heavy browsers or automation tools.  
- âœ… **Ethically safe** â€” works only on sites that *allow* scraping.  
- âœ… **Easily adaptable** â€” works for any page by changing a few selectors.  
- âœ… **Readable & beginner-friendly** â€” uses Pythonâ€™s most widely understood libraries.  

Instead of using complex tools like Selenium or Scrapy, this code relies on:
1. **`requests`** â†’ to make HTTP GET requests to fetch the raw HTML.  
2. **`BeautifulSoup`** â†’ to parse and extract specific data (titles, prices, etc.) from that HTML.  
3. **`PrettyTable`** â†’ to print results neatly in tabular format in the terminal.  
4. **`colorama`** â†’ to colorize output (errors, highlights, results) for better visibility in console logs.  

---

## ğŸ§© Dependencies â€” What You Need to Install

Before running the script, make sure you have Python 3.8+ installed.

Install the required dependencies using this command:

```bash
pip install requests beautifulsoup4 prettytable colorama

```
â–¶ï¸ Run the Python Script

After installing all dependencies, you can run the scraper easily using the following command:
```bash
 # Create virtual environment
 python -m venv venv
 # Activate virtual environment (Windows)
 venv\\Scripts\\activate
 # Install dependencies
 pip install selenium beautifulsoup4 pandas webdriver-manager requests lxml
 # Generate requirements.txt
 pip freeze &gt; requirements.txt
 # Run the Python Script
 python olx_scrape.py
